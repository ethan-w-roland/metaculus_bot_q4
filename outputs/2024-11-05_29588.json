{
    "ID": 29588,
    "title": "Will three or more Frontier AI Labs issue a joint statement committing to constrain their AI's capabilities before 2025?\n",
    "prompt": "QUESTION:\nWill three or more Frontier AI Labs issue a joint statement committing to constrain their AI's capabilities before 2025?\n\n\nTODAY'S DATE:\n2024-11-05\n\nRESOLUTION CRITERIA:\nThis question resolves as **YES** if before January 1, 2025, three or more Frontier AI Labs issue a joint statement committing to constrain their AI's capabilities, and **NO** otherwise.\n\nWe define a \"Frontier AI Lab\" as one who has trained models within one order of magnitude of the largest known model, which as of April 2023 is GPT-4. For example, as of April 2023, a frontier AI lab would be one which has trained a model with more than 2e24 FLOPs. However, this threshold will change as the largest known model changes.\n\nCompute levels will be established either per credible media reporting or reporting from a credible research institution such as [Epoch](https://epochai.org/) using a best guess point estimate.\n\nADDITIONAL CRITERIA:\n* A \"joint statement\" refers to a document that is explicitly authored or endorsed by each party. A technical paper does not qualify.\n\n* \"committing to constrain their AI's capabilities\" will be interpreted narrowly. For example, a commitment to prevent certain types of restricted speech, or a commitment to always do Reinforcement Learning with Human Feedback, will not qualify. The constraints must be on the general capabilities of the AIs - for example, committing to not using more than a certain amount of compute, or committing to not deploying a model until it verifiably cannot perform certain tasks, such as recursive self-improvement.\n\nBACKGROUND:\nAs of 2023-04-28, the Frontier Labs (defined in resolution criteria) are OpenAI, Anthropic, Microsoft, and Google Deepmind, [(source)](https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0). Labs that would join this with a ~10x growth in training compute include Meta, Yandex, Amazon, and Hugging Face.\n\n\"3 Frontier Labs\" here means that, at the time of the statement, at least 3 signatories are Frontier Labs according to the definition in the resolution criteria.\n\nADDITIONAL BACKGROUND:\nTo assess the likelihood of three or more Frontier AI labs issuing a joint statement committing to constrain their AI's capabilities before 2025, here are some relevant points:\n\n## Current Commitments and Collaborations\nAs of 2024, there has been significant collaboration among major AI labs on safety and responsible development. For example, 16 AI tech companies, including those from the US, China, and the Middle East, have agreed to safety commitments that include publishing safety frameworks and ensuring that risks associated with their models are mitigated.\n\n## Formation of Industry Bodies\nThe Frontier Model Forum, a new initiative by OpenAI, Microsoft, Google, and Anthropic, aims to promote the safe and responsible development of frontier AI systems. This includes advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.\n\n## Probability Assessments\nOn platforms like Manifold and Metaculus, the probability of three or more Frontier AI labs issuing a joint statement committing to constrain their AI's capabilities before 2026 is estimated to be around 16%. While this specific question is about 2026, it suggests that such joint commitments are not highly likely in the near future.\n\n## Regulatory and Safety Focus\nThere is a growing emphasis on safety and risk management in the development of frontier AI models. This includes commitments to not develop or deploy models if risks cannot be adequately mitigated, and to refine approaches alongside scientific advancements.\n\nGiven these points, while there is a clear trend towards increased collaboration and commitment to safety among Frontier AI labs, the specific likelihood of three or more labs issuing a joint statement to constrain their AI's capabilities before 2025 is not clearly answered as a yes or no. The current data and commitments suggest it is possible but not highly probable.\n\n### Base Rates and Reference Classes\n- The base rate for such joint statements in the past is not well-established, but recent collaborations indicate a growing trend towards joint commitments.\n- The reference class includes other instances of industry-wide agreements on safety and regulation, which are becoming more common in the AI sector.\n\n### Time to Resolution\n- The timeframe is relatively short, with the question focusing on actions before 2025.\n\n### Current State of the World\n- There is increasing pressure from governments and the public for AI labs to ensure the safe development and deployment of AI models.\n- The formation of industry bodies and agreements on safety commitments indicates a move towards more coordinated and responsible AI development.\n\nRELEVANT SUBQUESTIONS:\n\nSUBQUESTION: Are there currently at least three active Frontier AI Labs as defined in the resolution criteria?\nANSWER: Based on the provided sources, there is evidence to suggest that there are indeed multiple labs and companies actively involved in Frontier AI development, although the sources do not explicitly state the exact number of \"Frontier AI Labs\" as defined by specific resolution criteria.\n\n- The sources mention several prominent companies and labs, such as OpenAI, Anthropic, DeepMind, and Meta, which are all involved in advanced AI research and development.\n- These companies are described as pursuing \"bleeding-edge AI research\" and are highlighted as key players in the development of highly capable general-purpose AI models.\n\nGiven this information, it is reasonable to infer that there are at least three active entities (OpenAI, Anthropic, and DeepMind) that fit the description of being involved in Frontier AI development.\n\n**YES**\n\n---\n\nSUBQUESTION: Have any Frontier AI Labs previously issued joint statements committing to constrain their AI's capabilities according to the narrow definition provided?\nANSWER: While the sources do not provide a specific instance of Frontier AI labs issuing a joint statement committing to constrain their AI's capabilities in a narrow definition, they do mention commitments and frameworks that involve cooperation and safety measures.\n\nFor example, the \"Frontier AI Safety Commitments\" mentioned in the METR document involve 16 companies agreeing to evaluate their models for severe risks and implement necessary risk mitigations. This includes frameworks from companies like Anthropic, OpenAI, and Google DeepMind, which outline various safety measures and evaluations for their models.\n\nHowever, these commitments are more focused on general safety and risk mitigation rather than a narrow definition of constraining capabilities. Therefore, based on the provided sources, it is unclear if there has been a joint statement specifically committing to constrain AI capabilities according to a narrow definition.\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Has the Frontier Model Forum or similar industry groups released statements committing to constrain AI capabilities?\nANSWER: To determine whether the Frontier Model Forum or similar industry groups have released statements committing to constrain AI capabilities, we need to examine recent statements, policies, or public announcements from these groups.\n\n1. **Frontier Model Forum**: This is a group of major AI research organizations, including Google, Microsoft, and Anthropic, among others. As of my last update, there have been discussions and agreements among these companies regarding the responsible development and deployment of AI. However, specific commitments to constrain AI capabilities would need to be explicitly stated in their public communications or policy documents.\n\n2. **Similar Industry Groups**: Other industry groups, such as the Partnership on AI or the AI Now Institute, often focus on ethical AI development and may have released guidelines or statements on responsible AI practices. These could include constraints on certain types of AI capabilities.\n\nGiven the lack of specific, up-to-date sources in this response, here is the conclusion:\n\n- If there are no explicit statements or commitments found in the available sources, the answer would be \"UNCLEAR.\"\n\n### Answer:\nUNCLEAR\n\n---\n\nSUBQUESTION: Are there any known agreements or collaborations between three or more Frontier AI Labs on limiting AI capabilities?\nANSWER: While there are discussions and proposals for collaboration and regulation in the development and deployment of Frontier AI, there is no clear evidence of specific, known agreements or collaborations between three or more Frontier AI labs solely focused on limiting AI capabilities.\n\nHowever, there are several indications of collaborative efforts and proposals for joint actions:\n\n- There is mention of cooperation between private interests in the AI industry, including information-sharing, joint AI safety research ventures, and voluntary agreements to abide by shared safety standards.\n- The White House has compiled voluntary commitments, and there are references to the Bletchley Declaration from the first global AI Safety Summit, which suggest a collaborative approach to AI safety and evaluation.\n- Experts and policymakers are advocating for multi-stakeholder processes to develop and update safety standards for responsible frontier AI development and deployment.\n\nThese collaborations and proposals are more focused on ensuring safety, security, and responsible development rather than specifically limiting AI capabilities. Therefore, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Have any Frontier AI Labs adopted internal policies to constrain their AI's capabilities as defined?\nANSWER: Yes, several Frontier AI labs and companies have adopted internal policies to constrain their AI's capabilities.\n\nFor example, companies like Google DeepMind, OpenAI, and Anthropic have released detailed public commitments and policies that include various elements to mitigate risks associated with their AI models. These policies include:\n\n- **Covered Threat Models**: Identifying potential risks such as biological weapons development, cyberattacks, or autonomous behavior.\n- **Model Deployment Mitigations**: Implementing measures to reduce the risks associated with deploying AI models.\n- **Conditions for Halting Development and Deployment**: Establishing criteria for when to halt or pause the development and deployment of AI models based on evaluated risks.\n- **Full Capability Elicitation during Evaluations**: Conducting thorough evaluations to assess the capabilities and risks of AI models.\n\nThese policies, such as Google DeepMind’s Frontier Safety Framework, OpenAI’s Preparedness Framework, and Anthropic’s Responsible Scaling Policy, outline specific procedures and thresholds to ensure the safe development and deployment of frontier AI models.\n\n---\n\nSUBQUESTION: Are there any planned meetings or conferences before 2025 where Frontier AI Labs might issue joint statements?\nANSWER: Based on the provided sources, there are several conferences and meetings planned before 2025 that could potentially involve joint statements or collaborations, although they may not specifically mention Frontier AI Labs.\n\n1. **Global Conference on Frontier AI**: This conference, co-hosted by the Brookings Institution and European University Viadrina, will bring together diverse perspectives on AI governance, risks, and societal implications. While it does not specifically mention Frontier AI Labs, it is a platform where joint statements or collaborations could be discussed.\n\n2. **AI for Good Global Summit 2024**: Organized by the International Telecommunication Union (ITU) and other UN agencies, this summit focuses on practical applications of AI to advance global development priorities. It includes collaboration efforts and could be a venue for joint statements, although Frontier AI Labs is not explicitly mentioned.\n\n3. **The AI Conference 2024**: This conference convenes researchers, engineers, and business leaders in AI and includes various tracks and sessions. While it does not specify Frontier AI Labs, it is a significant gathering where joint statements or collaborations might be discussed among the attendees.\n\nGiven that these conferences are focused on AI and involve multiple stakeholders, it is plausible that joint statements could be issued during these events, even though Frontier AI Labs is not explicitly mentioned in the sources.\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Has there been increased regulatory pressure on AI labs to limit their AI capabilities before 2025?\nANSWER: Yes, there has been increased regulatory pressure on AI labs to limit and regulate their AI capabilities, particularly in the European Union, with implications that will extend into 2025.\n\n- The EU AI Act, which came into effect on August 1, 2024, imposes a risk-based framework that categorizes AI applications into different risk levels, with stricter regulations and bans on those considered high-risk or unacceptable.\n- Developers and deployers of AI systems, especially those classified as high-risk, are required to comply with specific obligations such as risk management, testing, technical robustness, data governance, transparency, human oversight, and cybersecurity before deployment.\n- Enforcement of the EU AI Act will commence in stages, with key deadlines in 2025, including the appointment of market surveillance authorities by member states and the implementation of various rules and codes of practice.\n\nIn the United States, while comprehensive regulations are not yet in place, there has been significant activity, including an Executive Order that mandates various federal agencies to address AI-related risks and ensure compliance with federal law, which also adds to the regulatory pressure on AI labs.\n\n---\n\nSUBQUESTION: Have any Frontier AI Labs recently expressed intentions to collaborate on AI safety measures involving capability constraints?\nANSWER: Yes, several frontier AI labs have recently expressed intentions to collaborate on AI safety measures, including those involving capability constraints.\n\n- The Frontier Model Forum, which includes multiple AI labs, has launched workstreams focused on developing best practices for AI safety evaluations and frameworks. These efforts include assessing and mitigating risks across various threat domains, such as biological threats and cyber vulnerabilities, and developing shared understandings around common elements of safety frameworks.\n- The AI Seoul Summit’s Frontier AI Safety Commitments, signed by sixteen companies, obligate these companies to assess risks and implement necessary mitigations across the AI lifecycle. This includes evaluating and mitigating capabilities that could pose severe risks, such as those related to biological weapons development or cyberattacks.\n- Los Alamos National Laboratory and OpenAI are collaborating on an evaluation study to assess the safety of frontier AI models, particularly in the context of biological threats and cyber vulnerabilities. This collaboration aims to establish a framework for evaluating current and future models to ensure responsible development and deployment.\n- The AISI (Artificial Intelligence Safety Institute) is also involved in research collaborations on safety case sketches, which include evaluating and mitigating risks from loss of control and autonomy in advanced AI models. These safety cases are tailored to specific technical and deployment contexts and involve capability evaluations and red team validation.\n\nThese collaborations and commitments indicate a clear intention to work together on AI safety measures, including those related to capability constraints.\n\n---\n\nSUBQUESTION: Have major AI incidents or failures occurred recently that might motivate labs to constrain AI capabilities?\nANSWER: Yes, there have been several major AI incidents and failures in recent years that could motivate labs to constrain AI capabilities.\n\n- The release of OpenAI's ChatGPT, despite being rushed to market, led to several issues including data breaches, production of toxic content, and misuse for scams. This has prompted regulatory investigations and public backlash.\n- Various chatbots have provided incorrect or harmful information, such as Air Canada's chatbot giving incorrect refund information, a Chevrolet chatbot agreeing to sell a car for $1, and New York City's MyCity chatbot advising businesses to engage in illegal activities.\n- AI-powered systems have exhibited significant biases, for example, Amazon's hiring algorithm downgrading women's applications and a risk assessment algorithm in US courts being biased against Black defendants.\n- Safety concerns have been raised with self-driving cars, such as Tesla's Autopilot crashing into emergency vehicles, highlighting the need for better regulation and testing.\n- The LAION 5B incident involved the introduction of Child Sexual Abuse Materials into the data pipeline due to indiscriminate web scraping, re-victimizing CSAM victims.\n\nThese incidents underscore the need for more stringent regulations, better data management, and enhanced testing to prevent such failures and ensure AI systems are developed responsibly.\n\n---\n\nSUBQUESTION: Is there significant public or governmental demand for AI labs to limit their AI capabilities?\nANSWER: There is a notable demand for regulation and oversight of AI capabilities, both from the public and governmental perspectives, although it may not be specifically framed as a demand to \"limit\" AI capabilities in a broad sense.\n\n### Public Demand\n- Surveys and polls indicate that a significant portion of the public is concerned about the risks associated with AI. For example, a 2023 Reuters/Ipsos poll found that 61% of Americans agree that AI poses risks to humanity, and a Fox News poll showed that a majority of Americans believe it is important for the federal government to regulate AI.\n\n### Governmental Demand\n- Governments are actively considering and implementing regulations to manage the risks and benefits of AI. In the United States, President Joe Biden signed an executive order requiring developers of AI systems to share safety test results with the government and to ensure AI systems are safe, secure, and trustworthy.\n- There are ongoing efforts to develop frameworks and legislation, such as the Blueprint for An AI Bill of Rights and a bipartisan legislative framework, which aim to establish principles for the safe and responsible use of AI.\n- The EU's AI Act, which is in the final stages of passing, includes extensive rules to regulate AI, including prohibiting uses that pose unacceptable risks.\n\n### Regulatory Focus\n- The focus is more on ensuring that AI systems are developed and used in a way that is safe, transparent, and trustworthy, rather than on limiting their capabilities outright. Regulations often aim to mitigate risks, ensure accountability, and promote transparency and public trust in AI systems.\n\nIn summary, while there is a strong demand for regulation and oversight to ensure the safe and responsible development and use of AI, it is not necessarily about limiting AI capabilities but rather about managing the associated risks and benefits. Therefore, the answer is:\n\n**YES**, there is significant public and governmental demand for regulating AI, but it is more about ensuring safety, transparency, and accountability rather than outright limitation.",
    "comment": "(a) **Time left until the outcome to the question is known**: Approximately **57 days** remain until January 1, 2025.\n\n(b) **What the outcome would be if nothing changed**: If nothing changed, the outcome would be **NO**, as no such joint statement has been issued to date.\n\n(c) **What you would forecast if there was only a quarter of the time left**: With only about **14 days** left, I would forecast a lower probability, around **5%**.\n\n(d) **What you would forecast if there was 4x the time left**: With approximately **228 days** left, I would forecast a higher probability, around **20%**.\n\n---\n\n**Rationale:**\n\n- **Current Status**: As of November 5, 2024, there is no public record of three or more Frontier AI Labs issuing a joint statement committing to constrain their AI's capabilities as per the narrow definition provided.\n\n- **Time Constraints**: With less than two months remaining until the deadline, the likelihood of such a statement being issued diminishes due to the limited time for coordination and agreement among the labs.\n\n- **Historical Precedence**: While there have been collaborative efforts on AI safety and responsible AI development (e.g., the Frontier Model Forum and various safety commitments), these have not specifically involved a joint statement to constrain AI capabilities in the narrow sense defined.\n\n- **Regulatory Pressure and Public Demand**: There is increasing regulatory pressure and public demand for AI labs to ensure the safe and responsible development of AI. Governments have enacted legislation and executive orders aiming to manage AI risks. However, these factors have not yet resulted in the specific type of joint commitment required for a **YES** resolution.\n\n- **Motivating Factors**:\n\n  - **AI Incidents**: Recent AI incidents and failures have raised concerns about AI risks, potentially motivating labs to consider capability constraints.\n  \n  - **Safety Collaborations**: Labs are engaging in safety collaborations, such as developing safety frameworks and evaluation processes, indicating a willingness to work together on AI governance.\n\n- **Challenges**:\n\n  - **Coordination Difficulty**: Aligning at least three Frontier AI Labs to agree on and publicly issue a joint statement with specific commitments is challenging, especially within the short remaining timeframe.\n  \n  - **Competitive Interests**: Labs may be reluctant to constrain their capabilities due to competitive pressures and market dynamics.\n  \n  - **Narrow Definition**: The requirement for the commitment to be about constraining general AI capabilities (not just safety practices or specific applications) sets a high bar that has not been met so far.\n\n- **Likelihood Assessment**:\n\n  - Given the time left and the absence of any announced plans for such a joint statement, the probability remains low.\n  \n  - If there were upcoming conferences or events focused on AI governance before the end of 2024, the probability might be slightly higher, but no such events are indicated in the provided information.\n\n- **Time Sensitivity**:\n\n  - **With Less Time**: If only a quarter of the time remained, the probability would decrease further due to even tighter time constraints.\n  \n  - **With More Time**: If there were four times as much time left, extending well into 2025, the probability would increase, reflecting more opportunities for such collaborations to materialize.\n\n---\n\n**Probability: 10%**",
    "probability": 10
}