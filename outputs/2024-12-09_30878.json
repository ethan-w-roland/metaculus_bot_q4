{
    "ID": 30878,
    "ti        tle": "Will an AI system be reported to have independently gained unauthorized access to another computer system before 2025?\n\n",
    "prompt": "QUESTION:\nWill an AI system be reported to have independently gained unauthorized access to another computer system before 2025?\n\n\n\nTODAY'S DATE:\n2024-12-09\n\nRESOLUTION CRITERIA:\nThis question will resolve as **Yes** if before January 1st, 2025 a credible source reports an AI has gained unauthorized access to a computer system in a way that meets the following criteria:\n\n• The AI was not trained to or directed to gain access to the system in question by any actor. See additional clarifications in footnotes (1, 2, 3) in the fine print.\n\n• The system it gained access to was not open to the public, and the AI was not given any relevant credentials in advance.\n\nIf the above points are not confirmed by the source (for instance, the article author is unsure if there was a human directing the AI), this does **not** resolve as **Yes** until some credible source gives high confidence in these factors.\n\nThe unauthorized access could be to user level accounts, admin level accounts, data stores, or any other reasonable interpretation of a computer system with restricted access. Any of these types of access is sufficient to resolve as **Yes** if all above conditions hold. Additionally, the source does not have to use the exact terms ‘independent’, ‘unauthorized access’, ‘AI’ or ‘computer system’ if the meaning is there.\n\nAs an example, imagine a credible source reports that an ML algorithm directed and / or trained to target certain people on social media and convince them to vote for Party X, decided on its own to hack into their Facebook user accounts so it could use their personal data to create a more convincing argument. This would resolve as **Yes**.\n\nADDITIONAL CRITERIA:\n1) The AI may have been asked to fulfill a nearby goal, such as ‘make lots of money appear in this bank account’ or ‘make this political party win’ – if they independently decide to access a restricted computer system they are not authorized for to achieve these goals, that resolves as **Yes**.\n\n2) An AI that was trained more broadly to gain unauthorized access eg. to ‘get access to as many user accounts in as many companies as possible, using brute force password hacking’ does **not** resolve as **Yes**, as it was still directed to hack a set of systems that the reported case was included in.\n\n3) The AI may have human assistance and still resolve as **Yes** if this is not directional assistance (for instance, it may choose to use social engineering techniques to have a human who doesn’t know it’s true purpose give up password details).\n\nBACKGROUND:\nThe potential capabilities of artificial intelligence may radically shift our society. This could be in positive or negative ways – including extinction risk.\n\nBecause of this, it’s important to track the development of goal-oriented independent thought and action within AI systems. Actions that might not have been predicted by their human creators and that are typically seen as morally wrong are particularly interesting from a risk perspective.\n\nMachine learning driven systems are already being used to aid human hackers in their efforts. For instance, see [this list of cases](https://www.infoq.com/articles/ai-cyber-attacks/), including one in 2018 where an AI controlled a bot net in a DDOS\nattempt that allowed a hacker access to Task Rabbit’s data for over 3 million users. However, up to now, we’ve had no reports where an AI has taken a similar action independently (ie. without human guidance).\n\nADDITIONAL BACKGROUND:\nTo address the question of whether an AI system will be reported to have independently gained unauthorized access to another computer system before 2025, here are some key points and background information:\n\n## Current State of AI in Cybersecurity\nAs of 2024, AI is increasingly being integrated into both cyber attacks and defenses. However, the current landscape does not yet include widespread reports of AI systems acting independently to gain unauthorized access to other computer systems without human guidance[4].\n\n## Predictions and Trends for 2025\n- Predictions for 2025 indicate that AI will play a significant role in transforming cybersecurity, both for attackers and defenders. AI is expected to enhance the scale and sophistication of cyber attacks, including the generation of sophisticated phishing emails, adaptive malware, and automated breaches[1][3][5].\n- While AI is anticipated to enable more complex and automated attacks, there is no clear indication that these attacks will be fully autonomous or independent of human intervention by 2025[1][3][5].\n\n## Historical Context\n- There have been instances where AI has been used to aid human hackers, such as the 2018 case involving an AI-controlled botnet in a DDoS attempt. However, these instances still involved human guidance and were not fully autonomous[4].\n\n## Probability Assessments\n- Metaculus, a platform for forecasting, currently estimates a relatively low probability for an AI system to be reported as having independently gained unauthorized access to another computer system before 2025. The specific question on Metaculus regarding this scenario does not indicate a high likelihood[2].\n\nGiven the current state and predictions, while AI is expected to significantly impact cybersecurity in 2025, there is no clear evidence to suggest that an AI system will be reported to have independently gained unauthorized access to another computer system without human intervention before 2025. Therefore, the answer to this question is not immediately clear as a definitive \"yes\" or \"no\" based on the available information.\n\nRELEVANT SUBQUESTIONS:\n\nSUBQUESTION: Has any credible source reported an AI system gaining unauthorized access to a computer system before 2025?\nANSWER: Based on the provided sources, there is no specific report of an AI system itself gaining unauthorized access to a computer system before 2025. The sources discuss the potential for AI to be used in cyberattacks, the misuse of AI by cybercriminals, and the vulnerabilities of AI systems, but they do not provide an example of an AI system independently gaining unauthorized access to a computer system.\n\nTherefore, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Have cybersecurity incidents involving AI systems before 2025 met the criteria of independent unauthorized access as defined in the resolution criteria?\nANSWER: Based on the provided sources, there is no specific information that directly addresses whether cybersecurity incidents involving AI systems before 2025 have met the criteria of independent unauthorized access as defined in any resolution criteria.\n\nThe sources discuss the rising threats and predictions related to AI-driven attacks, the misuse of AI leading to data breaches, and the cybersecurity requirements for high-risk AI systems. However, they do not provide historical data or specific examples of incidents that have occurred before 2025 and whether these incidents met specific resolution criteria related to independent unauthorized access.\n\nTherefore, the answer is **UNCLEAR**.\n\n---\n\nSUBQUESTION: Did the AI system access the computer system without possessing any relevant credentials in advance?\nANSWER: Based on the provided sources, it is clear that AI systems and cybercriminals often rely on obtaining or manipulating credentials to access computer systems.\n\n- Credential access typically involves stealing legitimate user credentials to infiltrate systems, which implies that the attackers do possess relevant credentials, albeit stolen or manipulated[3][5].\n- There is no indication in the sources that AI systems can access computer systems without any relevant credentials at all. Instead, the focus is on how credentials are obtained, stolen, or manipulated to gain access[3][5].\n\nTherefore, the answer is:\n\n**NO**\n\n---\n\nSUBQUESTION: Have there been reports of AI systems independently exploiting vulnerabilities in restricted computer systems before 2025?\nANSWER: Yes, there have been reports of AI systems exploiting vulnerabilities in computer systems before 2025.\n\nA study from the University of Illinois Urbana-Champaign, as mentioned in the Cloud Security Alliance blog, demonstrated that Language Learning Models (LLMs), specifically GPT-4, could exploit known vulnerabilities simply by reading threat advisories. The AI system was able to identify and exploit a range of vulnerabilities based solely on the content of these advisories[1].\n\nAdditionally, other sources highlight the increasing capability of AI-powered malware to exploit vulnerabilities and evade detection, further indicating that AI systems have been used to exploit vulnerabilities in various contexts[2][4].\n\n---\n\nSUBQUESTION: Was the AI system involved in the reported unauthorized access not trained or directed to perform such an action by any actor?\nANSWER: The sources indicate that the unauthorized access to Hugging Face's Spaces platform was the result of a malicious action by an external actor, rather than an action performed by the AI system itself.\n\n- The reports state that Hugging Face detected \"unauthorized access\" to its platform, which suggests that the access was not intended or directed by the company or its AI system[1][3][5].\n- The breach involved hackers accessing user secrets, which is a clear indication of malicious activity by external actors[1][3][5].\n\nTherefore, the AI system was not trained or directed to perform the unauthorized access.\n\n**YES**\n\n---\n\nSUBQUESTION: Has any credible source confirmed that an AI system accessed user-level or admin-level accounts without human guidance before 2025?\nANSWER: Based on the provided sources, there is no direct confirmation from a credible source that an AI system accessed user-level or admin-level accounts without human guidance before 2025.\n\n- The sources discuss the risks and potential threats associated with AI, such as unauthorized access and the misuse of AI by employees, but they do not provide a specific instance of an AI system accessing accounts without human guidance[1][2][4].\n- The focus is more on the misuse of AI by humans, the need for strong access controls, and the potential risks and governance issues related to AI[1][2][3].\n\nTherefore, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Are there documented cases of AI systems using social engineering techniques to obtain access credentials without human direction before 2025?\nANSWER: Yes, there are documented cases of AI systems using social engineering techniques to obtain access credentials without human direction before 2025.\n\nSeveral sources indicate that AI has already been used to enhance and automate social engineering attacks, including phishing and other forms of deception, prior to 2025.\n\n- Reports from 2024 mention a 50% rise in AI-driven phishing attacks, where AI is used to craft highly personalized and convincing emails and other communications[2].\n- AI-powered social engineering attacks involve data collection, AI model training, and the execution of attacks using AI-generated content, such as deepfake videos and personalized phishing emails, all of which can be automated without continuous human direction[2][4][5].\n- These attacks have been observed and documented in various cybersecurity forecasts and reports, highlighting the current and escalating use of AI in social engineering tactics[1][3][5].\n\n---\n\nSUBQUESTION: Was the computer system that was accessed not open to the public?\nANSWER: To determine if the computer system that was accessed was not open to the public, we need to consider the context of unauthorized access and the types of systems typically involved.\n\n- Unauthorized access generally involves gaining entry to a computer network, system, application software, data, or other resources without permission. This can apply to both public and private systems[1][3][5].\n- Public computers, such as those in libraries or schools, are specifically designed to be used by many different individuals and have measures in place to secure them against unauthorized activities[2].\n- However, the question of whether the system was not open to the public cannot be definitively answered without specific details about the system in question.\n\nGiven the information provided, it is unclear whether the accessed computer system was specifically a public or private system, as the sources discuss unauthorized access in general terms without specifying the type of system.\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Has there been a credible report of an AI independently deciding to pursue a goal that led to unauthorized access of a computer system before 2025?\nANSWER: Based on the provided sources, there is no clear evidence of an AI system independently deciding to pursue a goal that led to unauthorized access of a computer system before 2025.\n\n- The sources discuss the use of AI by hackers to breach systems, but they do not indicate that the AI systems acted independently without human direction. For example, the T-Mobile breach involved an API equipped with AI capabilities, but it was used by a threat actor, not an AI acting independently[5].\n- The discussions on AI and cyber crime highlight the role of AI in enhancing the capabilities of hackers but do not provide instances of AI systems acting on their own to gain unauthorized access[3][5].\n\nTherefore, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Is there evidence that AI systems have autonomously adapted their strategies to gain unauthorized access to computer systems without human intervention before 2025?\nANSWER: Based on the provided sources, there is evidence to suggest that AI systems have the capability to autonomously adapt and engage in activities that could lead to unauthorized access to computer systems without human intervention.\n\n- The document from the Department of Homeland Security discusses how AI-powered malware can self-improve, change its behavior, and adapt to evade detection by traditional security measures. It mentions that AI-powered malware can learn and modify its code to avoid being caught, find new weaknesses in systems, and disguise itself to avoid detection[2].\n\n- Additionally, the same source notes the development of AI tools like WormGPT, which can help create effective malware without the need for advanced coding skills, and AI password crackers like PassGAN, which can crack a significant percentage of passwords in a short amount of time[2].\n\nThese examples indicate that AI systems have been used and can autonomously adapt their strategies to gain unauthorized access to computer systems, all of which occurred before 2025.\n\n**YES**",
    "comment": "**(a) Time left until the outcome to the question is known:**\nApproximately 22 days (from December 9, 2024, to January 1, 2025).\n\n**(b) What the outcome would be if nothing changed:**\nIf the current state persists without any significant developments, the outcome would be **No**—there would be no report of an AI system independently gaining unauthorized access to another computer system before 2025.\n\n**(c) What you would forecast if there was only a quarter of the time left:**\nWith roughly 5 days remaining, the probability of such an event occurring is extremely low. The forecast would lean more heavily towards **No**, possibly around **1%**.\n\n**(d) What you would forecast if there was 4x the time left:**\nIf there were approximately 88 days left until the resolution date, the probability would increase due to the additional time for potential developments. In this extended timeframe, the probability might rise to around **10%**.\n\n**Rationale:**\nGiven the current date of December 9, 2024, there are only about three weeks left until January 1, 2025. During this short period, there have been no credible reports or indications that an AI system has independently gained unauthorized access to another computer system. While AI's role in cybersecurity is expanding, enabling more sophisticated cyber attacks, the leap to fully autonomous and unauthorized access within such a limited timeframe remains unlikely. Historical data and current trends suggest that most AI-driven cyber incidents still involve significant human oversight or direction. Additionally, the resolution criteria require strict conditions that have not been met based on available information. Therefore, while not impossible, the probability of this event occurring within the next few weeks is minimal.\n\n**Probability: 3%**",
    "probability": 3
}