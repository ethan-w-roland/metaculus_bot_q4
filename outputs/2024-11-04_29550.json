{
    "ID": 29550,
    "title": "Will \"slaughterbots\" kill at least 50 people outside a military conflict by 2025?\n\n",
    "prompt": "QUESTION:\nWill \"slaughterbots\" kill at least 50 people outside a military conflict by 2025?\n\n\n\nTODAY'S DATE:\n2024-11-04\n\nRESOLUTION CRITERIA:\nThis question resolves positively if a credible media outlet reports at least 50 civilians have been killed by autonomous drones before January 1st 2025. The media coverage must conclude that the drones were not under complete human control during the attack(s).\n\nADDITIONAL CRITERIA:\nFor the purposes of this question, a military conflict is defined as an armed conflict including one or more governments, which includes any of the following situations:\n\n1. Armed conflict between states or nations (international war).\n\n2. Armed conflict between factions within a state where at least 1 side represents the pre-existing government (civil war).\n\n3. Armed conflict between a state military and a militia / terrorist group.\n\nFatalities resulting from militia / terrorist group attack on politicians / civil servants should count towards the total death count. \n\nThe devices do not have to be airborne. Terrestrial / aquatic slaughterbots will count.\n\nPeople do not need to be killed in a single location / single attack. A cumulative death toll of 50 will resolves positively.\n\nBACKGROUND:\n\"Slaughterbots\" is a term used to describe drones that use artificial intelligence and facial recognition to target people with lethal force.\n\nThe term was brought into the popular lexicon by an [arms-control advocacy video](https://www.youtube.com/watch?v=O-2tpwW0kmU) created in 2017 by the Future of Life Institute (FLI) and Stuart Russell, who is a professor of computer science at UC Berkeley.\n\nWhile defense analyst Paul Scharre criticized the video as fear mongering [\"science fiction,\"](https://spectrum.ieee.org/why-you-shouldnt-fear-slaughterbots) the team behind the campaign continue to argue that [\"autonomous weapons are potentially scalable weapons of mass destruction (WMDs); essentially unlimited numbers can be launched by a small number of people. \"](https://spectrum.ieee.org/why-you-should-fear-slaughterbots-a-response)\n\nMax Tegmark, a professor at MIT and president of FLI, also warned that these weapons could be used by cartels and political dissents to carry out [targeted assassinations.](https://thenextweb.com/news/slaughterbots-are-a-step-away-from-your-neighborhood-and-we-need-a-ban)\n\n[In 2021 a resolution to ban autonomous lethal weapons failed to pass at the United Nations.](https://www.cnbc.com/2021/12/22/un-talks-to-ban-slaughterbots-collapsed-heres-why-that-matters.html)\n\nADDITIONAL BACKGROUND:\nTo assess the likelihood of \"slaughterbots\" or lethal autonomous weapons systems (LAWS) causing at least 50 civilian casualties outside a military conflict by 2025, here are some key points to consider:\n\n## Current State of Technology\n- LAWS, including \"slaughterbots,\" are increasingly sophisticated, with the capability to identify, select, and attack targets without human intervention. This technology is already being developed and deployed in various forms, such as drones and loitering munitions.\n\n## Ethical and Regulatory Concerns\n- There are significant ethical and regulatory concerns surrounding the use of LAWS. International bodies, including the United Nations, have expressed concerns about the lack of meaningful human control in decisions to use lethal force, and the potential for these systems to be used by non-state actors or terrorists.\n\n## Potential for Misuse\n- The democratization of AI and drone technology means that even small extremist groups or individuals could potentially access and misuse these technologies. This increases the risk of autonomous weapons being used in non-military contexts, such as terrorist attacks.\n\n## Geopolitical Context\n- The ongoing conflict in Ukraine has accelerated the development and deployment of advanced drone technologies, including semi-autonomous and autonomous systems. This conflict serves as a testing ground for these technologies, which could eventually be adapted for other uses.\n\n## Risk Scenarios\n- There are hypothetical scenarios where AI systems, including \"slaughterbots,\" could be used to target civilians, either intentionally or due to malfunction. These scenarios highlight the potential for significant harm, especially if such systems fall into the wrong hands or are not properly regulated.\n\n## Regulatory Frameworks\n- While there are efforts to establish regulatory frameworks and international agreements to govern the use of LAWS, the current state of these efforts is incomplete, and it is unclear how effectively these frameworks can prevent the misuse of these technologies.\n\nGiven these points, while it is impossible to predict with certainty whether \"slaughterbots\" will kill at least 50 people outside a military conflict by 2025, the following can be inferred:\n\n- The technological capability for such incidents exists and is advancing rapidly.\n- The risk of misuse by non-state actors or in non-military contexts is a significant concern.\n- Regulatory efforts are ongoing but may not be sufficient to prevent all potential misuses.\n\nTherefore, the answer to whether \"slaughterbots\" will kill at least 50 people outside a military conflict by 2025 cannot be definitively determined as a simple \"yes\" or \"no,\" but the risk factors and current trends suggest that such an event is within the realm of possibility.\n\nRELEVANT SUBQUESTIONS:\n\nSUBQUESTION: Have any governments publicly disclosed the development or deployment of lethal autonomous drones intended for use outside military contexts?\nANSWER: Based on the provided sources, there is no clear evidence that any governments have publicly disclosed the development or deployment of lethal autonomous drones specifically intended for use outside military contexts.\n\nThe sources primarily discuss the development, deployment, and ethical concerns related to lethal autonomous weapons in military contexts. While there are mentions of potential uses by police forces and in border control, these are more speculative or based on advocacy concerns rather than official government disclosures.\n\nTherefore, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Is there evidence of a black market for autonomous weapon systems?\nANSWER: Yes, there is evidence suggesting the potential for a black market for autonomous weapon systems.\n\n- The article from the Future of Life Institute mentions that once significant military powers begin manufacturing autonomous weapons, these systems are likely to proliferate and could soon appear on the black market, potentially ending up in the hands of terrorists, dictators, and warlords.\n\n- The Geneva Academy's report on autonomous weapons systems proliferation risks also discusses the diversion of these weapons from military stockpiles and their sale on the black market as one of the forms of proliferation.\n\nThese sources indicate that there is a recognized risk and potential for autonomous weapon systems to be traded on the black market.\n\n---\n\nSUBQUESTION: Have terrorist groups used drones to carry out attacks on civilians?\nANSWER: YES.\n\nTerrorist groups have indeed used drones to carry out attacks on civilians and other targets. Here are some examples:\n\n- In 2018, an attempt was made to assassinate Venezuelan President Nicolás Maduro using commercial drones laden with explosives, which although failed to hit the target, caused injuries to soldiers and disrupted a public event.\n- ISIS and other terrorist groups have used drones to conduct attacks, including dropping modified grenades and carrying out explosive attacks, which have resulted in civilian casualties.\n- The use of drones by terrorist groups such as ISIS has been described as a significant threat, including attacks on fortified positions and civilian areas, causing fear and disruption among traditional societies.\n\n---\n\nSUBQUESTION: Are significant advancements being made in autonomous weapon technology that could enable slaughterbots by 2025?\nANSWER: YES, significant advancements are being made in autonomous weapon technology that could potentially enable the development and deployment of highly advanced autonomous weapons, including those that could be described as \"slaughterbots,\" by 2025 or soon thereafter.\n\nSeveral sources highlight the rapid progress in this field:\n\n- Technological advances are spurring the development of autonomous weapons systems that operate without meaningful human control, with countries like China, Israel, Russia, South Korea, Türkiye, the United Kingdom, and the United States investing heavily in these technologies.\n- Reports indicate that autonomous weapons with some capabilities are already in use in conflicts, such as in Ukraine, Israel and Palestine, and Libya.\n- The integration of AI into weaponry is transitioning from conceptual to practical applications, with examples including AI-equipped drones and AI-driven targeting systems.\n- Experts warn that the current pace of technological development could lead to rapid advancements in autonomous weapons, potentially resulting in systems that can make life-and-death decisions without human oversight.\n\nThese advancements suggest that the technological capability to develop highly autonomous and potentially lethal systems is rapidly evolving.\n\n---\n\nSUBQUESTION: Have international efforts to ban or regulate lethal autonomous weapons failed to achieve a consensus?\nANSWER: International efforts to ban or regulate lethal autonomous weapons have not yet achieved a consensus, although significant progress and support for such measures have been noted.\n\n- Despite widespread discussions and calls for action, including a UN General Assembly resolution in December 2023 supported by 152 countries, there is no legally binding international treaty or consensus on banning or regulating lethal autonomous weapons systems (LAWS) as of now.\n- The Convention on Certain Conventional Weapons (CCW) has held several meetings and discussions, but a consensus on a ban or regulatory framework has not been reached. The process is ongoing, with continued calls for negotiations and the establishment of a new international treaty.\n- While many countries and international organizations have expressed support for a ban or strict regulations, the lack of a unified agreement among all states and the complexity of verifying compliance with any potential ban or regulations remain significant challenges.\n\nTherefore, the answer is:\n\n**NO**, international efforts have not failed entirely, but they have not yet achieved a consensus.\n\n---\n\nSUBQUESTION: Is fully autonomous lethal drone technology currently accessible to non-state actors?\nANSWER: Yes, fully autonomous lethal drone technology is currently accessible to non-state actors to some extent.\n\n- Non-state armed groups have been using drones, often commercial or homemade models, for various military purposes, including surveillance, intelligence gathering, and precision strikes. While many of these drones are manually operated, there is evidence that some non-state actors have access to more advanced autonomous capabilities.\n\n- For example, the Turkish-made Kargu-2 drone has been used in fully autonomous mode by non-state actors in Libya, demonstrating the ability to hunt down and engage targets independently.\n\n- Additionally, the proliferation of open-source AI and the adaptability of commercial UAVs for lethal purposes have made it feasible for non-state actors to develop and deploy autonomous drones. This includes the potential for drone swarms and other advanced autonomous systems.\n\nTherefore, while the technology may not be universally available or widely deployed among all non-state actors, it is clear that some non-state groups have already acquired and utilized fully autonomous lethal drone capabilities.\n\n---\n\nSUBQUESTION: Have experts or credible organizations issued warnings about the imminent risk of slaughterbots being used outside military conflicts?\nANSWER: Yes, experts and credible organizations have issued warnings about the imminent risk of slaughterbots being used outside military conflicts.\n\n- The video \"Slaughterbots\" itself, produced by the Future of Life Institute and Stuart Russell, depicts scenarios where autonomous drones are used to attack civilians, including politicians and students, highlighting the potential for these weapons to be used in non-military contexts.\n- Experts like Stuart Russell and Noel Sharkey have warned about the dangers of autonomous weapons being used in various settings, including attacks on civilians and the destabilization of global security. They emphasize the potential for these weapons to be used for assassinations, destabilizing nations, and selectively killing particular groups.\n- The Campaign to Stop Killer Robots, an international coalition, has also called for a ban on autonomous weapons, citing the risks they pose to civilians and the potential for their misuse in various contexts.\n\nThese warnings underscore the concern that slaughterbots could be used in ways that extend beyond traditional military conflicts.\n\n---\n\nSUBQUESTION: Have there been any reported incidents of autonomous drones killing civilians outside military conflicts up to November 2024?\nANSWER: Based on the provided sources, there have been reported incidents of drone strikes that have resulted in civilian casualties, although the autonomy of the drones in these specific incidents is not always explicitly stated. Here are some relevant points:\n\n- In Ethiopia, drone strikes by the Ethiopian armed forces have caused significant civilian casualties, including an incident where a drone strike hit a primary school, killing seven people, and another where a drone strike hit a bus station, killing 13 civilians. However, these sources do not specify if these drones were autonomous.\n\n- In Burkina Faso and Mali, drone strikes by the Burkinabe armed forces resulted in civilian casualties, but again, the autonomy of these drones is not specified.\n\n- The sources do discuss the development and use of autonomous drones, but they do not provide clear evidence of autonomous drones specifically killing civilians outside of military conflicts up to November 2024.\n\nGiven the lack of explicit information on the autonomy of the drones in these incidents, the answer is:\n\n**UNCLEAR**\n\n---\n\nSUBQUESTION: Is the use of artificial intelligence in weapon systems increasing despite ethical concerns?\nANSWER: YES.\n\nThe use of artificial intelligence (AI) in weapon systems is indeed increasing, despite the ethical concerns associated with it. Here are some key points from the sources:\n\n- Major advances in AI technology have led to the growing sophistication of military weaponry, including Lethal Autonomous Weapon Systems (L.A.W.S.) that can identify, engage, and destroy targets without human control.\n\n- The deployment of AI in military equipment and operations is already a reality, with examples such as drones equipped with AI targeting systems in Ukraine and the Israeli Defence Force developing AI systems to identify targets in Gaza.\n\n- Technological developments in Autonomous Weapon Systems (AWS) are progressing rapidly, and these systems are being used in conflicts around the world, even though there are no multilateral regulations to control their use or transfer.\n\n- The increasing autonomy in military equipment and the lack of clear regulations or norms to govern the development and use of these systems further indicate that the use of AI in weapon systems is on the rise.\n\n- Despite the ethical, legal, and political concerns raised by the use of AI in weapons systems, the technology continues to advance and is expected to play a significant role in future military operations.\n\n---\n\nSUBQUESTION: Are current regulations insufficient to prevent the proliferation of slaughterbots to non-state actors?\nANSWER: The current regulations are indeed insufficient to prevent the proliferation of lethal autonomous weapons, including slaughterbots, to non-state actors. Here are some key points that support this conclusion:\n\n1. **Lack of International Agreement**: Despite discussions and resolutions, there is no internationally agreed-upon, legally binding prohibition on autonomous weapons. The UN Convention on Certain Conventional Weapons (CCW) has not yet established adequate restrictions to prevent the proliferation of these weapons.\n\n2. **Proliferation Risks**: The technology to create lethal autonomous weapons is increasingly accessible, and non-state actors can convert civilian devices like drones into lethal autonomous weapons using open-source AI and lightweight onboard processing. This makes global dissemination of such technology inevitable and difficult to control.\n\n3. **Insufficient Export Controls**: The current export controls, such as those under the Wassenaar arrangement, have not been adequately implemented to prevent the transfer of these systems to malicious non-state actors. Discussions at the Group of Government Experts (GGE) have highlighted the need for better controls but have not yet resulted in effective measures.\n\n4. **Technical Feasibility**: The technology for creating these weapons is advancing rapidly, and non-state actors, including terrorist groups, have already demonstrated the ability to deploy armed drones and modify commercial UAVs for military use.\n\nIn summary, the existing regulatory framework does not adequately address the risks of proliferation of lethal autonomous weapons to non-state actors, highlighting the need for more stringent and coordinated international measures.\n\nYES",
    "comment": "**(a) Time left until the outcome is known:**\n\n- Approximately **2 months** remaining until January 1, 2025.\n\n---\n\n**(b) What the outcome would be if nothing changed:**\n\n- If nothing changed, the outcome would be **negative**. There have been **no reported incidents** of \"slaughterbots\" killing at least 50 civilians outside military conflicts up to November 4, 2024.\n\n---\n\n**(c) Forecast if there was only a quarter of the time left:**\n\n- With only **0.5 months** (approximately **2 weeks**) left, the probability of the event occurring would be **very low**, given the limited time for such a significant incident to happen and be reported.\n\n---\n\n**(d) Forecast if there was 4x the time left:**\n\n- If there were **8 months** remaining until January 1, 2025, the probability would be **higher**, as there would be more time for the technology to proliferate and for such an incident to occur.\n\n---\n\n**Rationale:**\n\nConsidering the information available as of November 4, 2024:\n\n- **Current State:** There have been **no reported incidents** of \"slaughterbots\" or lethal autonomous weapons killing civilians outside military conflicts that would meet the resolution criteria.\n\n- **Technological Capability:** While the technology for autonomous lethal drones exists and is advancing, there is **no clear evidence** that such weapons have been used to cause mass casualties in civilian settings outside military conflicts.\n\n- **Regulatory Environment:** International regulatory efforts have not resulted in a binding agreement to ban or control lethal autonomous weapons. This lack of regulation could increase the risk over time but has not yet led to the widespread use of such weapons by non-state actors.\n\n- **Accessibility to Non-State Actors:** Although there is potential for non-state actors to access autonomous weapon technology, and there have been instances of drones used in attacks, the scale required to cause **at least 50 deaths** has not been observed.\n\n- **Time Frame:** With only **2 months** remaining, the window for such an event to occur and be reported by credible media is relatively short.\n\n- **Base Rate of Past Events:** Historically, there have been **few to no incidents** of autonomous weapons causing mass civilian casualties outside military conflicts.\n\n- **Potential for Rapid Change:** Despite the low probability, it's not **zero** due to the unpredictable nature of technological misuse and the potential for rapid developments or attacks.\n\n**Final Assessment:**\n\nGiven the above factors, while acknowledging the non-zero risk due to technological accessibility and the potential for misuse, the probability of \"slaughterbots\" killing at least 50 people outside a military conflict by January 1, 2025, is **very low**.\n\n---\n\n**Probability: 3%**",
    "probability": 3
}